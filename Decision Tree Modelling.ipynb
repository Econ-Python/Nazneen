{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a7f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from category_encoders import OrdinalEncoder\n",
    "from IPython.display import VimeoVideo\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d88a20",
   "metadata": {},
   "source": [
    "## few terms to know\n",
    "1. depth = no. of splits that we need to make to get to the pure node\n",
    "2. purity of node is defined by gini coefficient\n",
    "3. We nee to divide data into train and test split and then divide train data into training data and validation set\n",
    "4. Parametric Model: the type of equition for the model is fixed before even the model training starts like for linear model we already have how the equation of relationship between X and y will look like. We only estimate the already defined equation\n",
    "5. Decision tree is a non-parametric method of estimation\n",
    "\n",
    "## Why ordinal encoding works for Decision Tree\n",
    "Decision tree does not care about the distance between two categories but to which split gives more purity to the node\n",
    "\n",
    "simple decision: \n",
    "                 \n",
    "                 decision tree --- ordinal encoder\n",
    "\n",
    "                 linear/logistic regression --- onehot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feb638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size = 0.2, random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "model = make_pipeline( OrdinalEncoder(), DecisionTreeClassifier(random_state =42))\n",
    "# Fit model to training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b7a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_train, model.predict(X_train))\n",
    "acc_val = model.score(X_val, y_val)\n",
    "\n",
    "print(\"Training Accuracy:\", round(acc_train, 2))\n",
    "print(\"Validation Accuracy:\", round(acc_val, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80659f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## hyperparameter tuning ##############\n",
    "# models usually are not build it first step that they can be generalized and depth of the tree is\n",
    "# such a way it get max of purity in node, we will need to trip the tree to avoid such case\n",
    "tree_depth = model.named_steps[\"decisiontreeclassifier\"].get_depth()\n",
    "print(\"Tree Depth:\", tree_depth)\n",
    "\n",
    "depth_hyperparams = range(1,50,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6316f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for training and validation accuracy scores\n",
    "training_acc = []\n",
    "validation_acc = []\n",
    "\n",
    "for d in depth_hyperparams:\n",
    "    # Create model with `max_depth` of `d`\n",
    "    test_model = \n",
    "    # Fit model to training data\n",
    "    test_model.fit(X_train, y_train)\n",
    "    # Calculate training accuracy score and append to `training_acc`\n",
    "    training_acc.append(test_model.score(X_train, y_train))\n",
    "    # Calculate validation accuracy score and append to `training_acc`\n",
    "    validation_acc.append(test_model.score(X_val, y_val))\n",
    "\n",
    "print(\"Training Accuracy Scores:\", training_acc[:3])\n",
    "print(\"Validation Accuracy Scores:\", validation_acc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5637267",
   "metadata": {},
   "outputs": [],
   "source": [
    "## finding best max_depth parameter\n",
    "# Plot `depth_hyperparams`, `training_acc`\n",
    "plt.plot(depth_hyperparams , training_acc, label = \"training\")\n",
    "plt.plot(depth_hyperparams , validation_acc, label = \"validation\")\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.ylabel(\"accuracy score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593f7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select best param val and retrain the model \n",
    "# here 6 is supposed selected depth\n",
    "model = make_pipeline( \n",
    "        OrdinalEncoder(), \n",
    "        DecisionTreeClassifier(max_depth = 6, random_state =42))\n",
    "    # Fit model to training data\n",
    "model.fit(X_train, y_train)\n",
    "test_acc = model.score(X_test,y_test)\n",
    "print(\"Test Accuracy:\", round(test_acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e338e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger figure\n",
    "fig, ax = plt.subplots(figsize=(25, 12))\n",
    "# Plot tree\n",
    "plot_tree(\n",
    "    decision_tree= model.named_steps[\"decisiontreeclassifier\"],\n",
    "    feature_names= list(X_train.columns),\n",
    "    filled=True,  # Color leaf with class\n",
    "    rounded=True,  # Round leaf edges\n",
    "    proportion=True,  # Display proportion of classes in leaf\n",
    "    max_depth=3,  # Only display first 3 levels\n",
    "    fontsize=12,  # Enlarge font\n",
    "    ax=ax,  # Place in figure axis\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b2a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(X_train.columns)# model[\"onehot_encoder\"].get_feature_names()\n",
    "importances = model.named_steps[\"decisiontreeclassifier\"].feature_importances_\n",
    "\n",
    "print(\"Features:\", features[:3])\n",
    "print(\"Importances:\", importances[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43a6efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp = pd.Series(importances, index = features).sort_values()\n",
    "feat_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65703b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create horizontal bar chart\n",
    "feat_imp.plot(kind = 'barh')\n",
    "plt.xlabel(\"Gini Importance\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
